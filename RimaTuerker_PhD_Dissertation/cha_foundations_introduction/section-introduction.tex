\section*{}
The World Wide Web also known as Web has become one of the largest global collection of information %interconncete knowledge 
with over 1.5 billion websites\footnote{https://www.internetlivestats.com/total-number-of-websites}. The Web is a global network environment where documents and other Web sources are identified by Uniform Resource Locators (URLs)~\cite{berners1998uri}. The documents are interlinked by hyper links~\cite{jacobs2004architecture} and accessible over the Internet. This global document repository encompasses almost every topic of human interest. The usefulness of Web document access can be seen from the fact that Google processes over 40,000 search queries every second on average, which translates to over 3.5 billion searches per day and 1.2 trillion searches per year worldwide\footnote{https://www.internetlivestats.com/google-search-statistics/}. 

Furthermore, today, billions  of  users$^1$ access and even contribute to this massive information exchange platform. Due to the large number of contributions as well as the digitization of all areas the content of the Web is drastically multiplying~\cite{STCImprovedby}. %Thus, there arouses a necessity of automatic analyzing and processing techniques of such data in order to satisfy a particular need of information~\cite{STTopicMemory}.
As a result, the fields of Natural  Language  Processing (NLP)~\cite{Jurafsky:2009:SLP:1214993} evolves in parallel, which concern the automatic analyzing and processing of natural language documents in order to satisfy a particular need of information. In other words, NLP is a field of computer science, artificial intelligence, and computational linguistics whose techniques are specifically designed to extract meaningful information from natural language documents. Some examples of NLP applications are Spelling and Grammar Checking, Auto  completion, Text Categorization, Text Summarization, Information Retrieval, etc.   Among those applications, text classification is the fundamental one which has been proven to be useful in various applications, such as sentiment analysis, news feed filtering and categorization, etc.~\cite{STTopicMemory}.

%Importance necessity of classification Application
Text categorization (a.k.a text classification) aims to assign one or more predefined classes or categories to natural language documents based on the attributes of the each text document. The textual data can be anything ranging from a phrase, sentences, paragraphs or an entire document. %Due to recent advancement of NLP many researchers are now concerned with developing new applications by exploiting text classification methods Some examples of text classification... 
However, with the rapid growth of online platforms such as Facebook, Twitter, online forums, etc. the Web users are now generating more and more \textit{short text} data. A large body of daily generated content of the Web, such as short text messages,micro blog posts, search queries etc. have become an important form for individuals to share information~\cite{STTopicMemory}. The main characteristic of short text is that the length of the text is very limited which is no longer than 200 characters~\cite{song2014short}. Further, unlike other documents short texts are usually rather ambiguous and sparse. Due to the limited length of the text which is only several to a dozen words~\cite{song2014short} the ambiguity cannot be resolved by depending on the context. Further, since the context is rather limited feature extraction is rather difficult. Such characteristics pose several major challenges to short text categorization tasks. While conventional text classification methods such as Support Vector Machines (SVMs) have demonstrated their success in classifying long and well structured text, as e.g., news articles, in case of short text they seem to have a substandard performance~\cite{STTopicMemory}.


\par Recently, several deep learning approaches have been proposed for short text classification, which demonstrated remarkable performance in this task~\cite{sentiment/aaai/MaPC18,DBLP:conf/emnlp/ChenSBY17}. The two main advantages of these models for the classification task are that minimum effort is required for feature engineering and their classification performance is better in comparison to traditional text classification approaches~\cite{WeaklySupervised}. However, the requirement of large amounts of labeled data remains the main bottleneck for neural network based approaches~\cite{WeaklySupervised}. Acquiring labeled data for the classification task is costly and time-consuming. Especially, if the data to be labeled is of a specific domain then only a limited number of domain experts is able to label them correctly, which makes it a labor intensive task.

In this thesis, we are concerned with
 %For instance, Google processes over 40,000 search queries every second on average which translates to over 3.5 billion searches per day. Maybe also news feed info